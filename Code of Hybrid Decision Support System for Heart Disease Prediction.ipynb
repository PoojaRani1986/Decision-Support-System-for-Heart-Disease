{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing files\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Dataset\n",
    "\n",
    "data = pd.read_csv(\"cleaveland.csv\" ,header=0)\n",
    "data['AG']=data['AG'].astype(float)\n",
    "data['SX']=data['SX'].astype(float)\n",
    "data['TCP']=data['TCP'].astype(float)\n",
    "data['RBPR']=data['RBPR'].astype(float)\n",
    "data['SCL']=data['SCL'].astype(float)\n",
    "data['FBSR']=data['FBSR'].astype(float)\n",
    "data['RER']=data['RER'].astype(float)\n",
    "data['MHRA']=data['MHRA'].astype(float)\n",
    "data['EXIA']=data['EXIA'].astype(float)\n",
    "data['STDIE']=data['STDIE'].astype(float)\n",
    "data['SPE']=data['SPE'].astype(float)\n",
    "data['NMV']=data['NMV'].astype(float)\n",
    "data['THM']=data['THM'].astype(float)\n",
    "data.TAR[data.TAR == 2] = 1\n",
    "data.TAR[data.TAR == 3] = 1\n",
    "data.TAR[data.TAR == 4] = 1\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MICE Imputation\n",
    "from fancyimpute import IterativeImputer\n",
    "\n",
    "\n",
    "MICE_imputer = IterativeImputer()\n",
    "\n",
    "data1 = MICE_imputer.fit_transform(data)\n",
    "data = pd.DataFrame({'AG': data1[:, 0], 'SX': data1[:, 1],'TCP': data1[:, 2],'RBPR': data1[:, 3],'SCL': data1[:, 4],'FBSR': data1[:, 5],'RER': data1[:, 6],'MHRA': data1[:, 7],'EXIA': data1[:, 8],'STDIE': data1[:, 9],'SPE': data1[:, 10],'NMV': data1[:, 11],'THM': data1[:, 12],'TAR': data1[:, 13]})\n",
    "\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placing Prediction Variables in x and target variables in y\n",
    "x=data.drop('TAR',axis=1)\n",
    "y=data.TAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select features using Genetic Algorithm\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    " \n",
    "SEED = 2018\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "class GeneticSelector:\n",
    "    def __init__(self, estimator, n_gen, size, n_best, n_rand, \n",
    "                 n_children, mutation_rate):\n",
    "        # Estimator \n",
    "        self.estimator = estimator\n",
    "        # Number of generations\n",
    "        self.n_gen = n_gen\n",
    "        # Number of chromosomes in population\n",
    "        self.size = size\n",
    "        # Number of best chromosomes to select\n",
    "        self.n_best = n_best\n",
    "        # Number of random chromosomes to select\n",
    "        self.n_rand = n_rand\n",
    "        # Number of children created during crossover\n",
    "        self.n_children = n_children\n",
    "        # Probablity of chromosome mutation\n",
    "        self.mutation_rate = mutation_rate\n",
    "        \n",
    "        if int((self.n_best + self.n_rand) / 2) * self.n_children != self.size:\n",
    "            raise ValueError(\"The population size is not stable.\")\n",
    "    def initilize(self):\n",
    "        population = []\n",
    "        for i in range(self.size):\n",
    "            chromosome = np.ones(self.n_features, dtype=np.bool)\n",
    "            mask = np.random.rand(len(chromosome)) < 0.3\n",
    "            chromosome[mask] = False\n",
    "            population.append(chromosome)\n",
    "        return population\n",
    "    def fitness(self, population):\n",
    "        X, y = self.dataset\n",
    "        scores = []\n",
    "        for chromosome in population:\n",
    "            score = -1.0 * np.mean(cross_val_score(self.estimator, X[:,chromosome], y, \n",
    "                                                       cv=5, \n",
    "                                                       scoring=\"neg_mean_squared_error\"))\n",
    "            scores.append(score)\n",
    "        scores, population = np.array(scores), np.array(population) \n",
    "        inds = np.argsort(scores)\n",
    "        return list(scores[inds]), list(population[inds,:])\n",
    "    def select(self, population_sorted):\n",
    "        population_next = []\n",
    "        for i in range(self.n_best):\n",
    "            population_next.append(population_sorted[i])\n",
    "        for i in range(self.n_rand):\n",
    "            population_next.append(random.choice(population_sorted))\n",
    "        random.shuffle(population_next)\n",
    "        return population_next\n",
    "    def crossover(self, population):\n",
    "        population_next = []\n",
    "        for i in range(int(len(population)/2)):\n",
    "            for j in range(self.n_children):\n",
    "                chromosome1, chromosome2 = population[i], population[len(population)-1-i]\n",
    "                child = chromosome1\n",
    "                mask = np.random.rand(len(child)) > 0.5\n",
    "                child[mask] = chromosome2[mask]\n",
    "                population_next.append(child)\n",
    "        return population_next\n",
    "    def mutate(self, population):\n",
    "        population_next = []\n",
    "        for i in range(len(population)):\n",
    "            chromosome = population[i]\n",
    "            if random.random() < self.mutation_rate:\n",
    "                mask = np.random.rand(len(chromosome)) < 0.05\n",
    "                chromosome[mask] = False\n",
    "            population_next.append(chromosome)\n",
    "        return population_next\n",
    "    def generate(self, population):\n",
    "        # Selection, crossover and mutation\n",
    "        scores_sorted, population_sorted = self.fitness(population)\n",
    "        population = self.select(population_sorted)\n",
    "        population = self.crossover(population)\n",
    "        population = self.mutate(population)\n",
    "        # History\n",
    "        self.chromosomes_best.append(population_sorted[0])\n",
    "        self.scores_best.append(scores_sorted[0])\n",
    "        self.scores_avg.append(np.mean(scores_sorted))\n",
    "        \n",
    "        return population\n",
    "    def fit(self, X, y):\n",
    " \n",
    "        self.chromosomes_best = []\n",
    "        self.scores_best, self.scores_avg  = [], []\n",
    "        \n",
    "        self.dataset = X, y\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        population = self.initilize()\n",
    "        for i in range(self.n_gen):\n",
    "            population = self.generate(population)\n",
    "            \n",
    "        return self \n",
    "    \n",
    "    @property\n",
    "    def support_(self):\n",
    "        return self.chromosomes_best[-1]\n",
    " \n",
    "    def plot_scores(self):\n",
    "        plt.plot(self.scores_best, label='Best')\n",
    "        plt.plot(self.scores_avg, label='Average')\n",
    "        plt.legend()\n",
    "        plt.ylabel('Scores')\n",
    "        plt.xlabel('Generation')\n",
    "        plt.show()\n",
    "sel = GeneticSelector(estimator=LinearRegression(), \n",
    "                      n_gen=18, size=200, n_best=40, n_rand=40, \n",
    "                      n_children=5, mutation_rate=0.05)\n",
    "\n",
    "X=x.values\n",
    "Y=y.values\n",
    "#X.shape\n",
    "sel.fit(X, Y)\n",
    "population=sel.initilize()\n",
    "population\n",
    "\n",
    "sel.crossover(population)\n",
    "\n",
    "print (sel.support_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction variables selected using genetic algorithm \n",
    "prediction_var_ga=['SX','TCP','RER','MHRA','EXIA','STDIE','SPE','NMV','THM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding out optimal number of features using recursive feature elimination\n",
    "\n",
    "X=data[prediction_var_ga]\n",
    "Y=data.TAR\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "nof_list=np.arange(1,9)            \n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,Y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,Y_train)\n",
    "    score = model.score(X_test_rfe,Y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Recursive Feature Selection on features selected using genetic algorithm\n",
    "from sklearn.feature_selection import RFE\n",
    "X=data[prediction_var_ga]\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, 8)\n",
    "X_rfe = rfe.fit_transform(X,Y)  \n",
    "model.fit(X_rfe,Y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place selected features using hybrid GA+RFE approach in x\n",
    "prediction_var_garfe=['SX','TCP','RER','EXIA','STDIE','SPE','NMV','THM']\n",
    "x=data[prediction_var_garfe]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply scaling using standard saclar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaledfeatures=StandardScaler()\n",
    "x=scaledfeatures.fit_transform(x)\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply class balancing using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smt = SMOTE(random_state=1)\n",
    "x,y = smt.fit_sample(x,y)\n",
    "y.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluate classifiers\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics\n",
    "\n",
    "gnb=GaussianNB()\n",
    "Accuracy=cross_val_score(gnb,x,y,cv=10).mean()\n",
    "modelgnb=gnb.fit(x,y)\n",
    "print (\"Naive Bayes Accuracy=\", Accuracy)\n",
    "\n",
    "y_pred = cross_val_predict(modelgnb,x,y,cv=10)\n",
    "confusion=metrics.confusion_matrix(y,y_pred)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print (confusion)\n",
    "sensitivity = TP / float(FN + TP)\n",
    "\n",
    "print(\"Naive Bayes sensitivity=\",sensitivity)\n",
    "specificity = TN / (TN + FP)\n",
    "print(\"Naive Bayes specificity=\",specificity)\n",
    "\n",
    "\n",
    "Precision = TP / float(TP + FP)\n",
    "Recall = TP / float(TP + FN)\n",
    "F1 = 2*((Precision*Recall)/(Precision+Recall))\n",
    "print (\"Naive Bayes Precision=\",Precision)\n",
    "print (\"Naive BayesRecall=\", Recall)\n",
    "print (\"Naive Bayes FMeasure\", F1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sv=SVC(C=100,gamma=0.0001,kernel='rbf')\n",
    "Accuracy=cross_val_score(sv, x, y,cv=10).mean()\n",
    "print (\"SVM Accuracy=\", Accuracy)\n",
    "y_pred = cross_val_predict(sv,x,y,cv=10)\n",
    "confusion=metrics.confusion_matrix(y,y_pred)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print (confusion)\n",
    "sensitivity = TP / float(FN + TP)\n",
    "\n",
    "print(\"SVM sensitivity=\",sensitivity)\n",
    "specificity = TN / (TN + FP)\n",
    "print(\"SVM specificity=\",specificity)\n",
    "\n",
    "\n",
    "Precision = TP / float(TP + FP)\n",
    "Recall = TP / float(TP + FN)\n",
    "F1 = 2*((Precision*Recall)/(Precision+Recall))\n",
    "print (\"SVM Precision=\",Precision)\n",
    "print (\"SVM Recall=\", Recall)\n",
    "print (\"SVM FMeasure\", F1)\n",
    "\n",
    "\n",
    "LR=LogisticRegression()\n",
    "\n",
    "Accuracy=cross_val_score(LR,x,y,cv=10).mean()\n",
    "\n",
    "print (\"LR Accuracy=\", Accuracy)\n",
    "\n",
    "\n",
    "y_pred = cross_val_predict(LR,x,y,cv=10)\n",
    "confusion=metrics.confusion_matrix(y,y_pred)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print (confusion)\n",
    "sensitivity = TP / float(FN + TP)\n",
    "\n",
    "print(\"LR sensitivity=\",sensitivity)\n",
    "specificity = TN / (TN + FP)\n",
    "print(\"LR specificity=\",specificity)\n",
    "\n",
    "Precision = TP / float(TP + FP)\n",
    "Recall = TP / float(TP + FN)\n",
    "F1 = 2*((Precision*Recall)/(Precision+Recall))\n",
    "print (\"LR Precision=\",Precision)\n",
    "print (\"LR Recall=\", Recall)\n",
    "print (\"LR FMeasure\", F1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=100,random_state=1,bootstrap=True,criterion='entropy',max_depth=5,max_features=2,min_samples_leaf=9)\n",
    "Accuracy=cross_val_score(RF,x,y,cv=10).mean()\n",
    "\n",
    "print (\"RF Accuracy=\", Accuracy)\n",
    "y_pred = cross_val_predict(RF,x,y,cv=10)\n",
    "confusion=metrics.confusion_matrix(y,y_pred)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print (confusion)\n",
    "sensitivity = TP / float(FN + TP)\n",
    "\n",
    "print(\"RF sensitivity=\",sensitivity)\n",
    "specificity = TN / (TN + FP)\n",
    "print(\"RF specificity=\",specificity)\n",
    "\n",
    "\n",
    "Precision = TP / float(TP + FP)\n",
    "Recall = TP / float(TP + FN)\n",
    "F1 = 2*((Precision*Recall)/(Precision+Recall))\n",
    "print (\"RF Precision=\",Precision)\n",
    "print (\"RF Recall=\", Recall)\n",
    "print (\"RF FMeasure\", F1)\n",
    "\n",
    "adaboost=AdaBoostClassifier(n_estimators=10, random_state=0,learning_rate=1)\n",
    "Accuracy=cross_val_score(adaboost, x, y,cv=10).mean()\n",
    "print (\"Adaboost Accuracy=\", Accuracy)\n",
    "y_pred = cross_val_predict(adaboost,x,y,cv=10)\n",
    "confusion=metrics.confusion_matrix(y,y_pred)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print (confusion)\n",
    "sensitivity = TP / float(FN + TP)\n",
    "\n",
    "print(\"Adaboost sensitivity=\",sensitivity)\n",
    "specificity = TN / (TN + FP)\n",
    "print(\"Adaboost specificity=\",specificity)\n",
    "\n",
    "\n",
    "Precision = TP / float(TP + FP)\n",
    "Recall = TP / float(TP + FN)\n",
    "F1 = 2*((Precision*Recall)/(Precision+Recall))\n",
    "print (\"Adaboost Precision=\",Precision)\n",
    "print (\"Adaboost Recall=\", Recall)\n",
    "print (\"Adaboost FMeasure\", F1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Parameters for RandomizedSearchCV() Method\n",
    "est = RandomForestClassifier(n_jobs=-1)\n",
    "rf_p_dist={'max_depth':[3,5,10,None],\n",
    "              'n_estimators':[10,50,100,150,200,300,400,500],\n",
    "              'max_features':randint(1,9),\n",
    "               'criterion':['gini','entropy'],\n",
    "               'bootstrap':[True,False],\n",
    "               'min_samples_leaf':randint(1,10),\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define RandomizedSearchCV() Method\n",
    "def hypertuning_rscv(est, p_distr, nbr_iter,X,y):\n",
    "    rdmsearch = RandomizedSearchCV(est, param_distributions=p_distr,\n",
    "                                  n_jobs=-1, n_iter=nbr_iter, cv=9)\n",
    "    rdmsearch.fit(X,y)\n",
    "    ht_params = rdmsearch.best_params_\n",
    "    ht_score = rdmsearch.best_score_\n",
    "    return ht_params, ht_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypertuning of Random Forest\n",
    "rf_parameters, rf_ht_score = hypertuning_rscv(est, rf_p_dist, 40, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters obtained using Random Forest\n",
    "rf_parameters\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
